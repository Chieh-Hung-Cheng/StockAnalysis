{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown --id 1v7mZebFdVHJ__-q6T2agoSWZOto_kZZZ --output stock_data.xlsx\n",
        "!gdown --id 1ugJJFEEqCttE9lKjX-TYRsb3gHGCxHDm --output bda2022_mid_news_2019.csv\n",
        "!gdown --id 1lJa5jjLU-O9B__LA0D0aC2kZqDD7-DMs --output bda2022_mid_news_2020.csv\n",
        "!gdown --id 1PCqcelrk5VqcCQciaUBocdmAUbDNOPnu --output bda2022_mid_news_2021.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNvKj4QsZzcK",
        "outputId": "69adddba-f557-49d0-b80d-93601c66bb01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=8874df33b17999b7dad58af823663f6ad42370fa3078cb263974c994b483b8cd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1_iaay5i/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "Successfully installed gdown-4.4.0\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1v7mZebFdVHJ__-q6T2agoSWZOto_kZZZ\n",
            "To: /content/stock_data.xlsx\n",
            "100% 500M/500M [00:05<00:00, 98.2MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ugJJFEEqCttE9lKjX-TYRsb3gHGCxHDm\n",
            "To: /content/bda2022_mid_news_2019.csv\n",
            "100% 413M/413M [00:03<00:00, 120MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lJa5jjLU-O9B__LA0D0aC2kZqDD7-DMs\n",
            "To: /content/bda2022_mid_news_2020.csv\n",
            "100% 272M/272M [00:02<00:00, 112MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PCqcelrk5VqcCQciaUBocdmAUbDNOPnu\n",
            "To: /content/bda2022_mid_news_2021.csv\n",
            "100% 317M/317M [00:02<00:00, 124MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#turn csv to dataframe\n",
        "bda2022_mid_news_2019 = pd.read_csv(\"bda2022_mid_news_2019.csv\")\n",
        "bda2022_mid_news_2020 = pd.read_csv(\"bda2022_mid_news_2020.csv\")\n",
        "bda2022_mid_news_2021 = pd.read_csv(\"bda2022_mid_news_2021.csv\")"
      ],
      "metadata": {
        "id": "-BcTB9ncaHzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "path=\"/content/drive/MyDrive/110-2 big data/database\"\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "rN96lyFOBSpS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "82c8b34a-9b85-4127-f4ad-83b6eeabea22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2b6f4988d0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/110-2 big data/database\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/110-2 big data/database'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import datetime"
      ],
      "metadata": {
        "id": "Lt2O2igmcCpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bda2022_mid_news_2020)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVm-eGV7aypW",
        "outputId": "004dac53-c40c-45b9-fdeb-3204f06a17e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157045"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def checkfinance(content):\n",
        "    finance_related = ['彰銀','京城銀','台中銀','旺旺保','華票','台產','臺企銀','高雄銀','高雄銀甲特','聯邦銀','聯邦銀甲特','遠東銀','安泰銀','新產','中再保','第一保','統一證','三商壽','華南金','富邦金','富邦特','國泰金','國泰特','開發金','玉山金','元大金','兆豐金','台新金','台新戊特','新光金','國票金','永豐金','中信金','第一金','王道銀','上海商銀','合庫金','群益證','群益期']\n",
        "    for i in range(len(finance_related)):\n",
        "        if finance_related[i] in content:\n",
        "            return True\n",
        "        else:\n",
        "            continue\n",
        "    return False"
      ],
      "metadata": {
        "id": "kITDW3jiG4hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_pth = os.getcwd()\n",
        "pros_pth = os.path.join(base_pth, 'pros')\n",
        "\n",
        "finance_related = ['彰銀', '京城銀', '台中銀', '旺旺保', '華票', '台產', '臺企銀', '高雄銀', '高雄銀甲特', '聯邦銀', '聯邦銀甲特', '遠東銀', \\\n",
        "                   '安泰銀', '新產', '中再保', '第一保', '統一證', '三商壽', '華南金', '富邦金', '富邦特', '國泰金', '國泰特', '開發金', '玉山金', \\\n",
        "                   '元大金', '兆豐金', '台新金', '台新戊特', '新光金', '國票金', '永豐金', '中信金', '第一金', '王道銀', '上海商銀', '合庫金', '群益證', '群益期']\n",
        "\n",
        "strick_bound = ['金融', '金控']\n",
        "up_kwd = ['漲', '反彈', '揚', '買壓']\n",
        "down_kwd = ['跌', '失守', '觀望', '保守', '緩', '賣壓', '挫']\n",
        "mutual_forb = ['震盪', '止', '抗', '整理']\n"
      ],
      "metadata": {
        "id": "LEs24KundC3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_df(path):\n",
        "    return pd.read_csv(path)\n",
        "def df_to_csv(df, path):\n",
        "    df.to_csv(path, encoding='utf_8_sig')\n"
      ],
      "metadata": {
        "id": "QaQ5IlSHeVUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_news():\n",
        "    news2019 = csv_to_df('bda2022_mid_news_2019.csv')\n",
        "    news2020 = csv_to_df('bda2022_mid_news_2020.csv')\n",
        "    news2021 = csv_to_df('bda2022_mid_news_2021.csv')\n",
        "    news = pd.concat([news2019, news2020, news2021])\n",
        "    return news\n",
        "\n",
        "\n",
        "def check(title, content, keywords, prohibits, cons_tl=True, cons_ct=False):\n",
        "    string = ''\n",
        "    if cons_tl: string += title\n",
        "    if cons_ct and isinstance(content, str): string += content\n",
        "\n",
        "    for i in keywords:\n",
        "        if i in string:\n",
        "            if prohibits is not None:\n",
        "                for j in prohibits:\n",
        "                    if j in string:\n",
        "                        return False\n",
        "            return True\n",
        "        else:\n",
        "            continue\n",
        "    return False\n",
        "\n",
        "\n",
        "def news_filter(news, kws, prohs):\n",
        "    filtered = []\n",
        "    for idx, line in tqdm(news.iterrows()):\n",
        "        # string = line['title']\n",
        "        # if isinstance(line['content'], str): string+= line['content']\n",
        "        # if checkfinance(string): filtered.append(line)\n",
        "        if check(line['title'], line['content'], kws, prohs): filtered.append(line)\n",
        "\n",
        "    if len(filtered) != 0:\n",
        "        ret_news = pd.concat(filtered, axis=1)\n",
        "        ret_news = ret_news.T\n",
        "        return ret_news\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def gen_filtered_news():\n",
        "    news = get_news()\n",
        "    news = news_filter(news, finance_related, None)\n",
        "    news.to_csv('filtered_0.csv', encoding='utf_8_sig')\n",
        "    news = news_filter(news, strick_bound, None)\n",
        "    news.to_csv('filtered_1.csv', encoding='utf_8_sig')\n",
        "    up_news = news_filter(news, up_kwd, down_kwd + mutual_forb)\n",
        "    down_news = news_filter(news, down_kwd, up_kwd + mutual_forb)\n",
        "    up_news.to_csv('filtered_up.csv', encoding='utf_8_sig')\n",
        "    down_news.to_csv('filtered_down.csv', encoding='utf_8_sig')\n",
        "\n",
        "gen_filtered_news()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1WMisQrb3OO",
        "outputId": "9a5e055d-5876-4fce-ac48-135f8b7eaddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "587392it [00:36, 15998.15it/s]\n",
            "14420it [00:00, 17070.90it/s]\n",
            "4698it [00:00, 17524.82it/s]\n",
            "4698it [00:00, 16712.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rovOoA18gTWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def phraselst_to_json(phraselist):\n",
        "    with open(os.path.join(pros_pth, 'phraselist_{}.json'.format(datetime.now().strftime('%m%d%H%M'))), 'w') as file:\n",
        "        json.dump(phraselist, file, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n",
        "        print('Save Phrase List Complete')\n",
        "\n",
        "\n",
        "def json_to_phraselst(filename='phraselist.json'):\n",
        "    path = os.path.join(pros_pth, filename)\n",
        "    with open(path, 'r') as file:\n",
        "        dictlist = json.load(file)\n",
        "    ret_list = []\n",
        "    for elm in dictlist:\n",
        "        ret_list.append(Phrase(**elm))\n",
        "    print('Load Phrase List Complete')\n",
        "    return ret_list"
      ],
      "metadata": {
        "id": "ZnnIGS2ydOuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_freqs(sub_news):\n",
        "    tf_ctr = Counter()\n",
        "    df_ctr = Counter()\n",
        "    slices_lst = []\n",
        "    for idx, line in tqdm(sub_news.iterrows()):\n",
        "        string = line['title']\n",
        "        if isinstance(line['content'], str): string += line['content']\n",
        "        slices = split_sentence_to_phrase(string)\n",
        "        tf_ctr += Counter(slices)\n",
        "        df_ctr += Counter(set(slices))\n",
        "        slices_lst.append(slices)\n",
        "\n",
        "    sub_news['slices'] = slices_lst\n",
        "    return sub_news, tf_ctr, df_ctr\n",
        "    # for idx, line in tqdm(sub_news.iterrows):\n",
        "    #    tf = Counter(slices)\n",
        "    #    df = Counter(set(slices))\n",
        "\n",
        "\n",
        "class Phrase:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.name = None\n",
        "        self.tf_up = None\n",
        "        self.df_up = None\n",
        "        self.tf_down = None\n",
        "        self.df_down = None\n",
        "        self.N_up = None\n",
        "        self.N_down = None\n",
        "\n",
        "        self.__dict__.update(kwargs)\n",
        "        self.tf_all = self.tf_up + self.tf_down\n",
        "        self.df_all = self.df_up + self.df_down\n",
        "\n",
        "        self.N_ttl = self.N_up + self.N_down\n",
        "        for elm in ['up', 'down']:\n",
        "            self.calc_tfidf(elm)\n",
        "\n",
        "    def __str__(self):\n",
        "        frqstr = 'Phrase:{}\\nUP: tf={}, df={}, tfidf={}/ {}\\nDN: tf={}, df={}, tfidf={} / \\nALL: tf={}, df={} / {}\\n' \\\n",
        "            .format(self.name, self.tf_up, self.df_up, self.tfidf_up, self.N_up,\n",
        "                    self.tf_down, self.df_down, self.tfidf_down, self.N_down,\n",
        "                    self.tf_all, self.df_all, self.N_ttl)\n",
        "\n",
        "        return frqstr + '\\n' + '-' * 30\n",
        "\n",
        "    def calc_tfidf(self, lmttyp):\n",
        "        # tf-idf = (1+log(tf)) * log(N_ttl/df) ???\n",
        "        N_tmp = self.N_up if lmttyp == 'up' else self.N_down\n",
        "        if lmttyp == 'up':\n",
        "            self.tfidf_up = (1 + math.log(self.tf_up + 1e-4)) * math.log(self.N_ttl / self.df_all)\n",
        "        elif lmttyp == 'down':\n",
        "            self.tfidf_down = (1 + math.log(self.tf_down + 1e-4)) * math.log(self.N_ttl / self.df_all)\n",
        "\n",
        "\n",
        "def gen_phrase_lst(tf_up_ctr, df_up_ctr, tf_down_ctr, df_down_ctr, N_up, N_down):\n",
        "    phraselst = []\n",
        "    df_all_ctr = df_up_ctr + df_down_ctr\n",
        "    for name, times in df_all_ctr.most_common():\n",
        "        phraselst.append(Phrase(name=name,\n",
        "                                 tf_up=tf_up_ctr[name] if name in tf_up_ctr else 0,\n",
        "                                 df_up=df_up_ctr[name] if name in df_up_ctr else 0,\n",
        "                                 tf_down=tf_down_ctr[name] if name in tf_down_ctr else 0,\n",
        "                                 df_down=df_down_ctr[name] if name in df_down_ctr else 0,\n",
        "                                 N_up=N_up,\n",
        "                                 N_down=N_down))\n",
        "    return phraselst\n",
        "\n",
        "def adding_ctr():\n",
        "    up_news = csv_to_df(os.path.join(pros_pth, 'filtered_up.csv'))\n",
        "    down_news = csv_to_df(os.path.join(pros_pth, 'filtered_down.csv'))\n",
        "    up_news, tf_up_ctr, df_up_ctr = gen_freqs(up_news)\n",
        "    down_news, tf_down_ctr, df_down_ctr = gen_freqs(down_news)\n",
        "    phraselst = gen_phrase_lst(tf_up_ctr, df_up_ctr, tf_down_ctr, df_down_ctr, len(up_news), len(down_news))\n",
        "    phraselst_to_json(phraselst)\n",
        "    up_news.to_csv(os.path.join(pros_pth, 'filtered_up_ctr.slices'), encoding='utf_8_sig')\n",
        "    down_news.to_csv(os.path.join(pros_pth, 'filtered_down_slices.csv'), encoding='utf_8_sig')"
      ],
      "metadata": {
        "id": "dfiLA9R4dbpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}